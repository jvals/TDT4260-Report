\section{Background} % What does the reader need to know?

% Knowledge that is necessary to understand the proposed solution
% Who is the intened audience and what do they need to know for the
% work to be understood.

Moore's law states that the number of transistors on a CPU will double
every two years~\cite{moore}. Ideally, this would give us twice the
computation power on a system every 24 months. However, because of
limitations such as memory performance, this does not happen.

To better utilize the available processing power, we need to avoid CPU
stalling for memory operations. Stalls can occur because loading data
from main memory takes many cycles. We can avoid some of this waiting
by keeping often-used data in a special memory which is smaller and
faster than main memory, and closer to the processor. This memory is
called Cache memory.

Because we can't keep all the data of main memory in cache, we
inevitably get what is called cache misses. A cache miss means we did
not find what we were looking for in cache, and we need to look for
this data in main memory. Again, this can waste hundreds of
cycles. Therefore, we want to minimize the number of cache misses.

Prefetching is one way to decrease the number of cache misses. If we
are able to load relevant data into cache before it is needed, we
avoid CPU stalls. However, knowing what to prefetch is not easy. The
definition of a good prefetch is that the prefetched data is used by
the processor before it is
replaced~\cite{srinivasan_davidson_tyson_2004}. This means that
prefetched data which goes unused is bad, because this data requires
valuable bandwidth. Additionally, bad prefetches might evict addresses
from cache which could have been used by the processor in the near
future.

The strategy this paper explores is ``Reference Prediction Tables'', as
described in~\cite{chen_baer_1995}.

\subsection{Reference Prediction Tables}
In RPT, we keep a table with entries corresponding to load
instructions. Every time the program counter hits a load, the current
address (tag), as well as the memory location we load from
(previous address), is stored as an entry in this table. When we encounter a
tag which already resides in the table, we check what address we are
about to load from this time, and from that subtract the previous
address. The result of this subtraction is stored in the corresponding
entry as the ``stride''. The complete format of an entry is shown in
Figure~\ref{table:entry}.

\begin{table}
  \centering
  \begin{tabular}{ | c | c | c | c |}
    \hline
    Tag & Previous Address & Stride & State \\ \hline
  \end{tabular}
  \caption{RPT Entry}
  \label{table:entry}
\end{table}

% State machine
\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[node distance=3cm,on grid,auto]
   \node[state] (q_0)   {init};
   \node[state] (q_1) [right=of q_0] {steady};
   \node[state] (q_2) [below=of q_0] {transient};
   \node[state] (q_3) [right=of q_2] {no-pred};
    \path[->]
    (q_0) edge  [out=5, in=175] node {correct} (q_1)
          edge  node [swap, text width=2.3cm] {\hspace{0.4cm}incorrect
            (update stride)} (q_2)
    (q_1) edge [out=185, in=355, looseness=1] node {incorrect} (q_0)
          edge [loop right] node {correct} ()
    (q_2) edge  [out=355, in=185] node [swap, text width=2.3cm] {\hspace{0.4cm}incorrect (update stride)} (q_3)
          edge  node [swap] {correct} (q_1)
    (q_3) edge [out=175, in=5] node [swap] {correct} (q_2)
          edge  [loop right] node [text width=2.3cm] {\hspace{0.4cm}incorrect (update stride)} ();
\end{tikzpicture}
\caption{State machine as proposed in~\cite{chen_baer_1995}}
\label{figure:statemachine}
\end{center}
\end{figure}
