\section{Background} % What does the reader need to know?

% Knowledge that is necessary to understand the proposed solution
% Who is the intened audience and what do they need to know for the
% work to be understood.

Moore's law states that the number of transistors on a CPU will double
every two years~\cite{moore}. Ideally, this would give us twice the
computation power on a system every 24 months. However, because of
limitations such as memory performance, this does not happen.

To better utilize the available processing power, CPU stalling for
memory operations needs to be avoided. Stalls occur because loading
data from main memory takes several cycles. We can avoid some of this
waiting by keeping often-used data in a special memory which is
smaller and faster than main memory, and closer to the processor. This
memory is called Cache memory.

Because we can't keep all main memory data in cache, we inevitably get
what is called cache misses. A cache miss happens when the requested
data is not found in cache, requiring the program to fetch the data
from main memory. This can waste hundreds of cycles. Therefore, the
number of cache misses should be minimized.

Prefetching is one way to decrease the number of cache misses. By
loading relevant data into cache before it is needed, CPU stalls can
be avoided. However, knowing what to prefetch is not easy. The
definition of a good prefetch is that the prefetched data is used by
the processor before it is
replaced~\cite{srinivasan_davidson_tyson_2004}. This means that
prefetched data which goes unused is bad, because this data requires
valuable bandwidth. Additionally, bad prefetches might evict data from
cache which could have been used by the processor in the near future.

This paper explores ``Reference Prediction Tables'' and ``Global
History Buffers'', as described in~\cite{chen_baer_1995}
and~\cite{nesbit_smith_2005}.

