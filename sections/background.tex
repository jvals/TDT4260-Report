\section{Background} % What does the reader need to know?

% Knowledge that is necessary to understand the proposed solution
% Who is the intened audience and what do they need to know for the
% work to be understood.

% Moore's law states that the number of transistors on a CPU will double
% every two years~\cite{moore}. Ideally, this would give us twice the
% computation power on a system every 24 months. However, because of
% limitations such as memory performance, this does not happen.

Prefetching can decrease the number of cache misses. By loading
relevant data into cache before it is needed, CPU stalls can be
avoided. However, knowing what to prefetch is not easy. The definition
of a good prefetch is that the prefetched data is used by the
processor before it is
replaced~\cite{srinivasan_davidson_tyson_2004}. This means that
prefetched data which goes unused is bad, because this data requires
valuable space. Additionally, bad prefetches might evict data from
cache which could have been used by the processor in the near future.

\subsection{Sequential Prefetching}

As mentioned, the easiest prefetching scheme is to fetch the next
block in addition to the block the processor needs. This works well in
programs with linear flow, but it's often not enough to fetch a single
block. Increasing the prefetch degree, the number of blocks to
prefetch, can improve performance. However, if too many blocks are
fetched, the cache might evict useful blocks, which would lower the
performance. Another important parameter is prefetch distance, which
is the amount of blocks between the accessed address and the
prefetched address. This can be increased to improve performance if it
happens to meet the timing requirements. However, this is very task
specific.

\subsection{Stride Directed Prefetcher}

A Stride Directed Prefetcher (SDP) is an example of an instruction-based
prefetcher. Different methods are used to predict how big the next
stride is going to be, based on earlier memory accesses. Data
structures, like tables, are used to store previous addresses, which
causes some overhead both in computing and storage. However, a good
implementation can give a high speedup as we show later in this paper.

\subsection{Reference Prediction Table}

A Reference Prediction Table is an extension of a strided prefetcher,
which seeks to avoid bad prefetches. By introducing states,
prefetching can be delayed until the observed pattern is stable. If
the same stride is observed twice in a row, a prefetch should be
considered. Otherwise, no prefetching takes place for the requested
address.

\subsection{Global History Buffer}

A more general technique is to use a Global History Buffer (GHB),
where each entry has a pointer to the previous entry generated by the
same load address. It's general because it is normally combined with
other prefetching algorithms. Usually, the GHB is accompanied by a
smaller index buffer, which holds pointers to the most recent GHB
entry. The taxonomy introduced by~\cite{nesbit_smith_2005} uses the
pair X/Y to denote the methods used. X is the key used for accessing
the index table, and Y is the mechanism used for detecting addressing
patterns. The implementation described in this paper uses PC/DC, that
is, index lookup with the program counter as key, and delta
correlation for detecting access patterns.
